{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep-explanation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/InnovArul/nettutorial/blob/master/deep-ai-explanation/deep_explanation.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "ENoLhpY4xg16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "collapsed": true,
        "outputId": "21b71436-e0b2-4b91-d6e4-1a08f2a484a7"
      },
      "cell_type": "code",
      "source": [
        "# install torch\n",
        "!pip3 install torchvision\n",
        "\n",
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "# http://pytorch.org/\n",
        "from os import path\n",
        "import os\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "print(os.listdir('drive/'))\n",
        "\n",
        "# check torch\n",
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "# 4/AAArEUBolsikLH79NWhuPvgc_42PEFTdk_LSx5LaSgacoC4fFzh9q8Q\n",
        "# 4/AAD53xotTis4sv1mTFxudK8k9WlBQ0Ng81S2KMg-gjFu7RgQ_yP6ewo\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "['.Trash', 'mnist', 'Colab Notebooks', 'MoneyManager', 'SmsContactsBackup', 'visa rejection.pdf', 'Multiple View Geometry in Computer Vision (Second Edition).pdf', 'mvf-tutorial.pdf', 'Deep learning workshop (Responses).ods', 'jstock-fe78440e-e0fe-4efb-881d-264a01be483c-checksum=2251512174-date=1513596102632-version=1107.zip', 'sharathms.iit@gmail.com.png', 'Research Scholars Year Book 2016-2017_Files', 'Papers.ods', 'Contact Information (Responses).ods', 'Contact Information.zip', 'draft_chalearn-lap-2016_ECCVW2016.pdf', 'NIPS16-FAQ-Author.odt', 'Copy of NIPS16-FAQ-Author.odt', 'Assignment1-Data', 'IITMIC_application.docx.odt', 'Admissions.xlsx.ods', 'Admissions.xlsx', \"We're thinking.txt\", 'ways.cpp', 'Luckynumbers.cpp', 'Luckynumbers (27560356).cpp', 'mod8_randomcounter.PNG', 'amrita engineering entrance examination sample paper.pdf']\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KZ4ycVUI1pcQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "collapsed": true,
        "outputId": "f4c9586e-ee70-4d9f-a0df-f3ecfb15493f"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# import statements\n",
        "import torch, os, sys\n",
        "import torch.nn as nn, numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torchvision, torchvision.models as models\n",
        "import matplotlib.pyplot as plt, numpy as np\n",
        "from skimage import io, transform, morphology\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a219834156a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# import statements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo install torch, click the button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MS8REsWzy-SF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "dee012a2-54dc-414a-9bc6-8026b48bdf3a"
      },
      "cell_type": "code",
      "source": [
        "# hyper parameters\n",
        "image_path = ''\n",
        "kerne_size = 3\n",
        "n_blocks = 3\n",
        "n_layers = 3\n",
        "epochs = 1000\n",
        "lr = 0.01\n",
        "l1_coeff = 1.0\n",
        "center_of_mass = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rLMNZ7lT2B53",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "collapsed": true,
        "outputId": "3abeef26-591d-4209-bc48-0c58685787d8"
      },
      "cell_type": "code",
      "source": [
        "# helper functions\n",
        "def use_cuda():\n",
        "    return torch.cuda.is_available()\n",
        "\n",
        "def image_torch_to_np(img):\n",
        "    img = img.data.cpu().numpy()[0]\n",
        "    img = img.transpose((1,2,0))\n",
        "    return img\n",
        "\n",
        "# load the model\n",
        "def load_model(model_type='vgg16'):\n",
        "    model = models[model_type](pretrained=True)\n",
        "    model.eval()\n",
        "    \n",
        "    if use_cuda():\n",
        "        model = model.cuda()\n",
        "    \n",
        "    for p in model.features.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in model.classifier.parameters():\n",
        "        p.requires_grad = False\n",
        "       \n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "print(model)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-634f00088a21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-634f00088a21>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_type)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vgg16'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "LOHYMygJ4r_W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_image(img_path):\n",
        "    img = io.imread(img_path)\n",
        "    \n",
        "    if show_image:\n",
        "        fig = plt.figure('image')\n",
        "        fig.imshow(img)\n",
        "    \n",
        "    return img\n",
        "\n",
        "img_np = load_image(image_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fcSKTbE_JgFT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def image_np_to_torch(img):\n",
        "    resized_img = np.float32(transform.resize(img, (224, 224)))\n",
        "    means=[0.485, 0.456, 0.406]\n",
        "    stds=[0.229, 0.224, 0.225]\n",
        "    \n",
        "    for ch in range(3):\n",
        "        resized_img[:, :, ch] *= (means[ch] / stds[ch])\n",
        "    \n",
        "    resized_img = torch.from_numpy(resized_img.transpose((2,0,1))).unsqueeze(0)\n",
        "    if use_cuda():\n",
        "        resized_img = resized_img.cuda()\n",
        "    \n",
        "    return Variable(resized_img, requires_grad=True)\n",
        "\n",
        "img_torch = image_np_to_torch(img_np)\n",
        "print type(img_torch)\n",
        "print img_torch.shape\n",
        "print((img_torch.min(), img_torch.max()))\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Eoej8QNNA3p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "collapsed": true,
        "outputId": "95d5b3e1-6a27-4ce0-84ae-5fb41609f8f1"
      },
      "cell_type": "code",
      "source": [
        "# visualize scaling changes\n",
        "img_np_resized = img_torch_to_np(img_torch)\n",
        "img_avg = np.mean(img_np_resized, axis=2)\n",
        "\n",
        "plt.pcolormesh(img_avg)\n",
        "plt.colorbar()\n",
        "ax = plt.gca()\n",
        "ax.set_aspect(1)\n",
        "__ = plt.title(\"Vizualization of image transformed to torch tensor\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d5279460143f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_np_resized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_torch_to_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_torch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_np_resized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcolormesh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img_torch_to_np' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "XUDowmqPNc0i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def assess_input(input_img):\n",
        "    with open(\"data/imagenet1000_clsid_to_human.pkl\", \"r\") as fp:\n",
        "        vgg_class = pickle.load(fp)\n",
        "       \n",
        "    outputs = F.softmax(model(input_img))\n",
        "    outputs_np = outputs.data.cpu().numpy()\n",
        "    sorted_args = np.argsort(outputs_np[0, :])[::-1]\n",
        "    \n",
        "    print('top 5 classes')\n",
        "    print('(index) description : score')\n",
        "    \n",
        "    for i in sorted_args:\n",
        "        print('({}) {} : {}'.format(i, vgg_class[i], outputs_np[0,i]))\n",
        "    \n",
        "    if outputs_np[0, sorted_args[0]] < 0.5:\n",
        "        print(\"*** Warning ***\")\n",
        "        print(\"top category score under 0.5, extracted explanation may not be accurate on not well defined class\")\n",
        "\n",
        "\n",
        "vgg_input_assessment(img_torch, model)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4u2-Vhxm45LQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class PerturbationGenerator(nn.Module):\n",
        "    def __init__(self, kernel_size=3, n_blocks=3, n_layers=3):\n",
        "        self(PerturbationGenerator, self).__init__()\n",
        "        \n",
        "        # if kernel_size is even\n",
        "        if kernel_size % 2 == 1:\n",
        "            kernel_size = kernel_size + 1\n",
        "        \n",
        "        self.conv = nn.Conv2d(3, 3, kernel_size=kernel_size, padding=kernel_size//2)\n",
        "        self.n_blocks = n_blocks\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "    def forward(img):\n",
        "        # get scaling information\n",
        "        x_min = torch.min(img)\n",
        "        Dx_max = torch.max(img - x_min)\n",
        "        \n",
        "        # perform manipulation\n",
        "        for _ in range(self.n_blocks * self.n_layers):\n",
        "            x = self.conv(x)\n",
        "        \n",
        "        # rescale the manipulated output\n",
        "        x = (x - torch.min(x)) / torch.max(x) # x : 0 to 1\n",
        "        x = (x * Dx_max) + x_min # x : original xmin to xmax\n",
        "        \n",
        "        return x\n",
        "\n",
        "# create an instance\n",
        "img_perturber = PerturbationGenerator(kernel_size, n_blocks, n_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "64tQsgY-8U1M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_optimum_perturbation(epochs, model, perturber, img, lr=0.1, l1_coeff=0.01):\n",
        "    optimizer = optim.Adam(perturber.parameters(), lr=lr)\n",
        "    \n",
        "    # get the max output\n",
        "    output = F.softmax(model(img))\n",
        "    max_category = np.argmax(output.cpu().data.numpy())\n",
        "    print('category of highest probability', max_category, vgg_class[max_category])\n",
        "    losses = []\n",
        "    \n",
        "    # optimize the perturber for getting adversarial image\n",
        "    print('optimizing the perturber')\n",
        "    for _ in tqdm(range(epochs)):\n",
        "        perturbed_img = perturber(img)\n",
        "        output = F.softmax(model(perturbed_img))\n",
        "        img_diff = perturbed_img - img\n",
        "        loss = torch.mean(torch.abs(img_diff)) + output[0, max_category]\n",
        "        losses.append(loss.data[0])\n",
        "        \n",
        "        # optimize \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "   \n",
        "    # plot the loss\n",
        "    plt.figure(\"loss\")\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    \n",
        "    print \"original score: {}\".format(F.softmax()(vgg_model(img))[0, max_category])\n",
        "    print \"perturbed score: {}\".format(F.softmax()(vgg_model(perturbed_img))[0, max_category])\n",
        "    return perturbed_img\n",
        "\n",
        "perturbed_img = get_optimum_perturbation(\n",
        "    epochs, model, img_perturber, img_torch,\n",
        "    lr=lr, l1_coeff=l1_coeff,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OfLDPhrmG932",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "original_img_np = image_torch_to_np(img_torch)\n",
        "perturbed_img_np = image_torch_to_np(perturbed_img)\n",
        "\n",
        "mean_original_np = np.mean(original_img_np, axis=2)\n",
        "mean_perturbed_np = np.mean(perturbed_img_np, axis=2)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 5))\n",
        "fig.canvas.set_window_title(\"images\")\n",
        "\n",
        "im1 = ax1.pcolormesh(mean_original_np)\n",
        "fig.colorbar(im1, ax=ax1, fraction=0.046)\n",
        "ax1.set_aspect(1)\n",
        "ax1.set_title(\"mean original image\")\n",
        "\n",
        "im2 = ax2.pcolormesh(mean_perturbed_np)\n",
        "fig.colorbar(im2, ax=ax2, fraction=0.046)\n",
        "ax2.set_aspect(1)\n",
        "ax2.set_title(\"mean perturbed image\")\n",
        "\n",
        "fig.tight_layout()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EPigqEMIKcMd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def post_processing(original_img, perturbed_img):\n",
        "    original_img_np = image_torch_to_np(original_img, axis=2)\n",
        "    perturbed_img_np = image_torch_to_np(perturbed_img, axis=2)\n",
        "\n",
        "    mean_original_np = np.mean(original_img_np, axis=2)\n",
        "    mean_perturbed_np = np.mean(perturbed_img_np, axis=2)    \n",
        "    diff = (mean_original_np - mean_perturbed_np)**6\n",
        "    \n",
        "    # remove perturbations in the edges of image\n",
        "    h, w = np.shape(diff)\n",
        "    diff[:int(0.1 * h), :] = 0\n",
        "    diff[int(0.9 * h):, :] = 0\n",
        "    diff[:, :int(0.1 * w)] = 0\n",
        "    diff[:, int(0.9 * w):] = 0\n",
        "    \n",
        "    # dilate the important points left for visibility:\n",
        "    square = np.ones((20, 20))\n",
        "    diff = morphology.dilation(diff, square)\n",
        "    return diff, original_img_np, perturbed_img_np, mean_original_np, mean_perturbed_np\n",
        "    \n",
        "    \n",
        "diff, original_img_np, perturbed_img_np, mean_original_np, mean_perturbed_np = post_processing(img_torch, perturbed_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cIMmx2DyLgyM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_results(diff, original_img_np, perturbed_img_np, mean_original_np, mean_perturbed_np, indicate_center_of_mass=False):\n",
        "    center_mass = center_of_mass(diff)\n",
        "    \n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(15, 5))\n",
        "    \n",
        "    im1 = ax1.pcolormesh(mean_original_np)\n",
        "    fig.colorbar(im1, ax=ax1, fraction=0.046)\n",
        "    ax1.set_aspect(1)\n",
        "    ax1.set_title('original image')\n",
        "    \n",
        "    im2 = ax2.pcolormesh(mean_perturbed_np)\n",
        "    fig.colorbar(im2, ax=ax2, fraction=0.046)\n",
        "    ax2.set_aspect(1)\n",
        "    ax2.set_title('perturbed image')\n",
        "    \n",
        "    im3 = ax1.pcolormesh(diff)\n",
        "    fig.colorbar(im3, ax=ax3, fraction=0.046)\n",
        "    ax3.set_aspect(1)\n",
        "    ax3.set_title('differences')    \n",
        "    \n",
        "    if indicate_center_of_mass:\n",
        "        ax3.annotate(\"X: center of mass\", center_mass) \n",
        "        \n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_results(diff, original_img_np, perturbed_img_np, mean_original_np, mean_perturbed_np, indicate_center_of_mass=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "72RF2ZbvQNsx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a667f58-3358-4a77-da13-51659a5f4019"
      },
      "cell_type": "code",
      "source": [
        "def deep_explanation(image_path=\"drive/image_samples/cat2.jpg\",\n",
        "                    kernel_size=3,\n",
        "                    nblocks=2,\n",
        "                    nlayers=3,\n",
        "                    epochs=1000,\n",
        "                    lr=0.01,\n",
        "                    l1_coeff=1.0,\n",
        "                    indicate_center_of_mass=True,):\n",
        "    model = load_model()\n",
        "    print(model)\n",
        "\n",
        "    img_np = load_image(image_path)\n",
        "    img_torch = image_np_to_torch(img_np)\n",
        "    vgg_input_assessment(img_torch, model)\n",
        "    img_perturber = PerturbationGenerator(kernel_size, n_blocks, n_layers)\n",
        "    perturbed_img = get_optimum_perturbation(\n",
        "        epochs, model, img_perturber, img_torch,\n",
        "        lr=lr, l1_coeff=l1_coeff,\n",
        "    )\n",
        "\n",
        "    diff, original_img_np, perturbed_img_np, mean_original_np, mean_perturbed_np = post_processing(img_torch, perturbed_img)\n",
        "    plot_results(diff, original_img_np, perturbed_img_np, mean_original_np, mean_perturbed_np, indicate_center_of_mass=False)\n",
        "    "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: Not a git repository (or any of the parent directories): .git\r\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}